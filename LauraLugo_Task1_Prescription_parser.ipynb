{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lauralug0/GBC/blob/main/LauraLugo_Task1_Prescription_parser.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxBDhVN5bDE3"
      },
      "source": [
        "# Task 1: Create a Prescription Parser using CRF\n",
        "This task tests your ability to build a Doctor Prescription Parser with the help of CRF model\n",
        "\n",
        "Your job is to build a Prescription Parser that takes a prescription (sentence) as an input and find / label the words in that sentence with one of the already pre-defined labels\n",
        "\n",
        "### Problem: SEQUENCE PREDICTION - Label words in a sentence\n",
        "#### Input : Doctor Prescription in the form of a sentence split into tokens\n",
        "- Ex: Take 2 tablets once a day for 10 days\n",
        "\n",
        "#### Output : FHIR Labels\n",
        "- ('Take', 'Method')\n",
        "- ('2', 'Qty')\n",
        "- ('tablets', 'Form')\n",
        "- ('once', 'Frequency')\n",
        "- ('a', 'Period')\n",
        "- ('day', 'PeriodUnit')\n",
        "- ('for', 'FOR')\n",
        "- ('10', 'Duration')\n",
        "- ('days', 'DurationUnit')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTuDTBnSbDE5"
      },
      "source": [
        "### Major Steps\n",
        "- Install necessary library\n",
        "- Import the libraries\n",
        "- Create training data with labels\n",
        "    - Split the sentence into tokens\n",
        "    - Compute POS tags\n",
        "    - Create triples\n",
        "- Extract features\n",
        "- Split the data into training and testing set\n",
        "- Create CRF model\n",
        "- Save the CRF model\n",
        "- Load the CRF model\n",
        "- Predict on test data\n",
        "- Accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_m1FFDmbDE6"
      },
      "source": [
        "#### Install necesaary library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RtqBEEJabDE6",
        "outputId": "0bb978d2-14c2-4437-d46c-73c1c4bf313b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: sklearn-crfsuite in /usr/local/lib/python3.12/dist-packages (0.5.0)\n",
            "Requirement already satisfied: python-crfsuite>=0.9.7 in /usr/local/lib/python3.12/dist-packages (from sklearn-crfsuite) (0.9.11)\n",
            "Requirement already satisfied: scikit-learn>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from sklearn-crfsuite) (1.6.1)\n",
            "Requirement already satisfied: tabulate>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from sklearn-crfsuite) (0.9.0)\n",
            "Requirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.12/dist-packages (from sklearn-crfsuite) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.24.0->sklearn-crfsuite) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.24.0->sklearn-crfsuite) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.24.0->sklearn-crfsuite) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.24.0->sklearn-crfsuite) (3.6.0)\n",
            "Found existing installation: textacy 0.12.0\n",
            "Uninstalling textacy-0.12.0:\n",
            "  Successfully uninstalled textacy-0.12.0\n",
            "Found existing installation: spacy 3.7.2\n",
            "Uninstalling spacy-3.7.2:\n",
            "  Successfully uninstalled spacy-3.7.2\n",
            "Found existing installation: thinc 8.2.5\n",
            "Uninstalling thinc-8.2.5:\n",
            "  Successfully uninstalled thinc-8.2.5\n",
            "Found existing installation: networkx 2.8.8\n",
            "Uninstalling networkx-2.8.8:\n",
            "  Successfully uninstalled networkx-2.8.8\n",
            "Collecting spacy==3.7.2\n",
            "  Using cached spacy-3.7.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting textacy==0.12.0\n",
            "  Using cached textacy-0.12.0-py3-none-any.whl.metadata (4.8 kB)\n",
            "Collecting networkx<3.0\n",
            "  Using cached networkx-2.8.8-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy==3.7.2) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy==3.7.2) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy==3.7.2) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy==3.7.2) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy==3.7.2) (3.0.10)\n",
            "Collecting thinc<8.3.0,>=8.1.8 (from spacy==3.7.2)\n",
            "  Using cached thinc-8.2.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy==3.7.2) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy==3.7.2) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy==3.7.2) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.12/dist-packages (from spacy==3.7.2) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy==3.7.2) (0.9.4)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from spacy==3.7.2) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.12/dist-packages (from spacy==3.7.2) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from spacy==3.7.2) (2.32.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy==3.7.2) (2.11.10)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy==3.7.2) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy==3.7.2) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from spacy==3.7.2) (25.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.12/dist-packages (from spacy==3.7.2) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.12/dist-packages (from spacy==3.7.2) (1.26.4)\n",
            "Requirement already satisfied: cachetools>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from textacy==0.12.0) (5.5.2)\n",
            "Requirement already satisfied: cytoolz>=0.10.1 in /usr/local/lib/python3.12/dist-packages (from textacy==0.12.0) (1.1.0)\n",
            "Requirement already satisfied: jellyfish>=0.8.0 in /usr/local/lib/python3.12/dist-packages (from textacy==0.12.0) (1.2.1)\n",
            "Requirement already satisfied: joblib>=0.13.0 in /usr/local/lib/python3.12/dist-packages (from textacy==0.12.0) (1.5.2)\n",
            "Requirement already satisfied: pyphen>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from textacy==0.12.0) (0.17.2)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.12/dist-packages (from textacy==0.12.0) (1.16.3)\n",
            "Requirement already satisfied: scikit-learn>=0.19.0 in /usr/local/lib/python3.12/dist-packages (from textacy==0.12.0) (1.6.1)\n",
            "Requirement already satisfied: toolz>=0.8.0 in /usr/local/lib/python3.12/dist-packages (from cytoolz>=0.10.1->textacy==0.12.0) (0.12.1)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.12/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy==3.7.2) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy==3.7.2) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy==3.7.2) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy==3.7.2) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy==3.7.2) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.7.2) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.7.2) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.7.2) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.7.2) (2025.10.5)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.19.0->textacy==0.12.0) (3.6.0)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.12/dist-packages (from thinc<8.3.0,>=8.1.8->spacy==3.7.2) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.3.0,>=8.1.8->spacy==3.7.2) (0.1.5)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.12/dist-packages (from typer<0.10.0,>=0.3.0->spacy==3.7.2) (8.3.0)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.4.0,>=0.1.0->spacy==3.7.2) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy==3.7.2) (3.0.3)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy==3.7.2) (1.3.1)\n",
            "Using cached spacy-3.7.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.4 MB)\n",
            "Using cached textacy-0.12.0-py3-none-any.whl (208 kB)\n",
            "Using cached networkx-2.8.8-py3-none-any.whl (2.0 MB)\n",
            "Using cached thinc-8.2.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (865 kB)\n",
            "Installing collected packages: networkx, thinc, spacy, textacy\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "scikit-image 0.25.2 requires networkx>=3.0, but you have networkx 2.8.8 which is incompatible.\n",
            "nx-cugraph-cu12 25.6.0 requires networkx>=3.2, but you have networkx 2.8.8 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed networkx-2.8.8 spacy-3.7.2 textacy-0.12.0 thinc-8.2.5\n",
            "Collecting https://github.com/explosion/spacy-models/releases/download/-en_core_web_sm/-en_core_web_sm.tar.gz\n",
            "\u001b[31m  ERROR: HTTP error 404 while getting https://github.com/explosion/spacy-models/releases/download/-en_core_web_sm/-en_core_web_sm.tar.gz\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Could not install requirement https://github.com/explosion/spacy-models/releases/download/-en_core_web_sm/-en_core_web_sm.tar.gz because of HTTP error 404 Client Error: Not Found for url: https://github.com/explosion/spacy-models/releases/download/-en_core_web_sm/-en_core_web_sm.tar.gz for URL https://github.com/explosion/spacy-models/releases/download/-en_core_web_sm/-en_core_web_sm.tar.gz\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install nltk\n",
        "!pip install sklearn-crfsuite\n",
        "!pip uninstall -y textacy spacy thinc networkx\n",
        "!pip install -U spacy==3.7.2 textacy==0.12.0 \"networkx<3.0\"\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTYjDXhUbDE7"
      },
      "source": [
        "#### Import the necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dL1enEp6bDE7",
        "outputId": "122cf876-b61e-436e-a6e9-8431311c2d4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ],
      "source": [
        "import nltk\n",
        "import joblib\n",
        "import sklearn_crfsuite\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn_crfsuite import metrics\n",
        "from nltk import pos_tag\n",
        "\n",
        "# Download necessary NLTK data (if not already downloaded)\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MI0mZbeAbDE7"
      },
      "source": [
        "### Input data (GIVEN)\n",
        "#### Creating the inputs to the ML model in the following form:\n",
        "- sigs --> ['take 3 tabs for 10 days']       INPUT SIG\n",
        "- input_sigs --> [['take', '3', 'tabs', 'for', '10', 'days']]      TOKENS\n",
        "- output_labels --> [['Method','Qty', 'Form', 'FOR', 'Duration', 'DurationUnit']]       LABELS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "ePbEnStTbDE7"
      },
      "outputs": [],
      "source": [
        "sigs = [\"for 5 to 6 days\", \"inject 2 units\", \"x 2 weeks\", \"x 3 days\", \"every day\", \"every 2 weeks\", \"every 3 days\", \"every 1 to 2 months\", \"every 2 to 6 weeks\", \"every 4 to 6 days\", \"take two to four tabs\", \"take 2 to 4 tabs\", \"take 3 tabs orally bid for 10 days at bedtime\", \"swallow three capsules tid orally\", \"take 2 capsules po every 6 hours\", \"take 2 tabs po for 10 days\", \"take 100 caps by mouth tid for 10 weeks\", \"take 2 tabs after an hour\", \"2 tabs every 4-6 hours\", \"every 4 to 6 hours\", \"q46h\", \"q4-6h\", \"2 hours before breakfast\", \"before 30 mins at bedtime\", \"30 mins before bed\", \"and 100 tabs twice a month\", \"100 tabs twice a month\", \"100 tabs once a month\", \"100 tabs thrice a month\", \"3 tabs daily for 3 days then 1 tab per day at bed\", \"30 tabs 10 days tid\", \"take 30 tabs for 10 days three times a day\", \"qid q6h\", \"bid\", \"qid\", \"30 tabs before dinner and bedtime\", \"30 tabs before dinner & bedtime\", \"take 3 tabs at bedtime\", \"30 tabs thrice daily for 10 days \", \"30 tabs for 10 days three times a day\", \"Take 2 tablets a day\", \"qid for 10 days\", \"every day\", \"take 2 caps at bedtime\", \"apply 3 drops before bedtime\", \"take three capsules daily\", \"swallow 3 pills once a day\", \"swallow three pills thrice a day\", \"apply daily\", \"apply three drops before bedtime\", \"every 6 hours\", \"before food\", \"after food\", \"for 20 days\", \"for twenty days\", \"with meals\"]\n",
        "input_sigs = [['for', '5', 'to', '6', 'days'], ['inject', '2', 'units'], ['x', '2', 'weeks'], ['x', '3', 'days'], ['every', 'day'], ['every', '2', 'weeks'], ['every', '3', 'days'], ['every', '1', 'to', '2', 'months'], ['every', '2', 'to', '6', 'weeks'], ['every', '4', 'to', '6', 'days'], ['take', 'two', 'to', 'four', 'tabs'], ['take', '2', 'to', '4', 'tabs'], ['take', '3', 'tabs', 'orally', 'bid', 'for', '10', 'days', 'at', 'bedtime'], ['swallow', 'three', 'capsules', 'tid', 'orally'], ['take', '2', 'capsules', 'po', 'every', '6', 'hours'], ['take', '2', 'tabs', 'po', 'for', '10', 'days'], ['take', '100', 'caps', 'by', 'mouth', 'tid', 'for', '10', 'weeks'], ['take', '2', 'tabs', 'after', 'an', 'hour'], ['2', 'tabs', 'every', '4-6', 'hours'], ['every', '4', 'to', '6', 'hours'], ['q46h'], ['q4-6h'], ['2', 'hours', 'before', 'breakfast'], ['before', '30', 'mins', 'at', 'bedtime'], ['30', 'mins', 'before', 'bed'], ['and', '100', 'tabs', 'twice', 'a', 'month'], ['100', 'tabs', 'twice', 'a', 'month'], ['100', 'tabs', 'once', 'a', 'month'], ['100', 'tabs', 'thrice', 'a', 'month'], ['3', 'tabs', 'daily', 'for', '3', 'days', 'then', '1', 'tab', 'per', 'day', 'at', 'bed'], ['30', 'tabs', '10', 'days', 'tid'], ['take', '30', 'tabs', 'for', '10', 'days', 'three', 'times', 'a', 'day'], ['qid', 'q6h'], ['bid'], ['qid'], ['30', 'tabs', 'before', 'dinner', 'and', 'bedtime'], ['30', 'tabs', 'before', 'dinner', '&', 'bedtime'], ['take', '3', 'tabs', 'at', 'bedtime'], ['30', 'tabs', 'thrice', 'daily', 'for', '10', 'days'], ['30', 'tabs', 'for', '10', 'days', 'three', 'times', 'a', 'day'], ['take', '2', 'tablets', 'a', 'day'], ['qid', 'for', '10', 'days'], ['every', 'day'], ['take', '2', 'caps', 'at', 'bedtime'], ['apply', '3', 'drops', 'before', 'bedtime'], ['take', 'three', 'capsules', 'daily'], ['swallow', '3', 'pills', 'once', 'a', 'day'], ['swallow', 'three', 'pills', 'thrice', 'a', 'day'], ['apply', 'daily'], ['apply', 'three', 'drops', 'before', 'bedtime'], ['every', '6', 'hours'], ['before', 'food'], ['after', 'food'], ['for', '20', 'days'], ['for', 'twenty', 'days'], ['with', 'meals']]\n",
        "output_labels = [['FOR', 'Duration', 'TO', 'DurationMax', 'DurationUnit'], ['Method', 'Qty', 'Form'], ['FOR', 'Duration', 'DurationUnit'], ['FOR', 'Duration', 'DurationUnit'], ['EVERY', 'Period'], ['EVERY', 'Period', 'PeriodUnit'], ['EVERY', 'Period', 'PeriodUnit'], ['EVERY', 'Period', 'TO', 'PeriodMax', 'PeriodUnit'], ['EVERY', 'Period', 'TO', 'PeriodMax', 'PeriodUnit'], ['EVERY', 'Period', 'TO', 'PeriodMax', 'PeriodUnit'], ['Method', 'Qty', 'TO', 'Qty', 'Form'], ['Method', 'Qty', 'TO', 'Qty', 'Form'], ['Method', 'Qty', 'Form', 'PO', 'BID', 'FOR', 'Duration', 'DurationUnit', 'AT', 'WHEN'], ['Method', 'Qty', 'Form', 'TID', 'PO'], ['Method', 'Qty', 'Form', 'PO', 'EVERY', 'Period', 'PeriodUnit'], ['Method', 'Qty', 'Form', 'PO', 'FOR', 'Duration', 'DurationUnit'], ['Method', 'Qty', 'Form', 'BY', 'PO', 'TID', 'FOR', 'Duration', 'DurationUnit'], ['Method', 'Qty', 'Form', 'AFTER', 'Period', 'PeriodUnit'], ['Qty', 'Form', 'EVERY', 'Period', 'PeriodUnit'], ['EVERY', 'Period', 'TO', 'PeriodMax', 'PeriodUnit'], ['Q46H'], ['Q4-6H'], ['Qty', 'PeriodUnit', 'BEFORE', 'WHEN'], ['BEFORE', 'Qty', 'M', 'AT', 'WHEN'], ['Qty', 'M', 'BEFORE', 'WHEN'], ['AND', 'Qty', 'Form', 'Frequency', 'Period', 'PeriodUnit'], ['Qty', 'Form', 'Frequency', 'Period', 'PeriodUnit'], ['Qty', 'Form', 'Frequency', 'Period', 'PeriodUnit'], ['Qty', 'Form', 'Frequency', 'Period', 'PeriodUnit'], ['Qty', 'Form', 'Frequency', 'FOR', 'Duration', 'DurationUnit', 'THEN', 'Qty', 'Form', 'Frequency', 'PeriodUnit', 'AT', 'WHEN'], ['Qty', 'Form', 'Duration', 'DurationUnit', 'TID'], ['Method', 'Qty', 'Form', 'FOR', 'Duration', 'DurationUnit', 'Qty', 'TIMES', 'Period', 'PeriodUnit'], ['QID', 'Q6H'], ['BID'], ['QID'],['Qty', 'Form', 'BEFORE', 'WHEN', 'AND', 'WHEN'], ['Qty', 'Form', 'BEFORE', 'WHEN', 'AND', 'WHEN'], ['Method', 'Qty', 'Form', 'AT', 'WHEN'], ['Qty', 'Form', 'Frequency', 'DAILY', 'FOR', 'Duration', 'DurationUnit'], ['Qty', 'Form', 'FOR', 'Duration', 'DurationUnit', 'Frequency', 'TIMES', 'Period', 'PeriodUnit'], ['Method', 'Qty', 'Form', 'Period', 'PeriodUnit'], ['QID', 'FOR', 'Duration', 'DurationUnit'], ['EVERY', 'PeriodUnit'], ['Method', 'Qty', 'Form', 'AT', 'WHEN'], ['Method', 'Qty', 'Form', 'BEFORE', 'WHEN'], ['Method', 'Qty', 'Form', 'DAILY'], ['Method', 'Qty', 'Form', 'Frequency', 'Period', 'PeriodUnit'], ['Method', 'Qty', 'Form', 'Frequency', 'Period', 'PeriodUnit'], ['Method', 'DAILY'], ['Method', 'Qty', 'Form', 'BEFORE', 'WHEN'], ['EVERY', 'Period', 'PeriodUnit'], ['BEFORE', 'FOOD'], ['AFTER', 'FOOD'], ['FOR', 'Duration', 'DurationUnit'], ['FOR', 'Duration', 'DurationUnit'], ['WITH', 'FOOD']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VqayCT4jbDE7",
        "outputId": "d571420f-100c-4402-ad4a-b15409de0cae"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(56, 56, 56)"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ],
      "source": [
        "len(sigs), len(input_sigs) , len(output_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRy_9ljzbDE8"
      },
      "source": [
        "### Creating a Tuples Maker method\n",
        "Create the tuples as given below by writing a function **tuples_maker(input_sigs, output_labels)** and returns **output** as given below\n",
        "\n",
        "Input(s):\n",
        "- input_sigs\n",
        "- output_lables\n",
        "\n",
        "Output:\n",
        "\n",
        "[[('for', 'FOR'),\n",
        "  ('5', 'Duration'),\n",
        "  ('to', 'TO'),\n",
        "  ('6', 'DurationMax'),\n",
        "  ('days', 'DurationUnit')], [second sentence], ...]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "7POZGYp7bDE8"
      },
      "outputs": [],
      "source": [
        "def tuples_maker(inp, out):\n",
        "\n",
        "    sample_data = []\n",
        "\n",
        "    for tokens, labels in zip(inp, out):\n",
        "        sentence_tuples = [(t, l) for t, l in zip(tokens, labels)]\n",
        "        sample_data.append(sentence_tuples)\n",
        "\n",
        "    return sample_data\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6aITwkmbDE8"
      },
      "source": [
        "### Creating the triples_maker( ) for feature extraction\n",
        "- input: tuples_maker_output\n",
        "- output:\n",
        "[[('for', 'IN', 'FOR'),\n",
        "  ('5', 'CD', 'Duration'),\n",
        "  ('to', 'TO', 'TO'),\n",
        "  ('6', 'CD', 'DurationMax'),\n",
        "  ('days', 'NNS', 'DurationUnit')], [second sentence], ... ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "9gvUT2U6bDE8"
      },
      "outputs": [],
      "source": [
        "def triples_maker(whole_data):\n",
        "    from nltk import pos_tag\n",
        "    sample_data = []\n",
        "\n",
        "\n",
        "    for sentence in whole_data:\n",
        "\n",
        "        tokens = [token for token, _ in sentence]\n",
        "        labels = [label for _, label in sentence]\n",
        "\n",
        "\n",
        "        pos_tags = [pos for _, pos in pos_tag(tokens)]\n",
        "\n",
        "\n",
        "        triples = [(tok, pos, lab) for tok, pos, lab in zip(tokens, pos_tags, labels)]\n",
        "\n",
        "\n",
        "        sample_data.append(triples)\n",
        "\n",
        "    return sample_data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XaHv4EN4bDE9"
      },
      "source": [
        "### Creating the features extractor method (GIVEN as a BASELINE)\n",
        "#### The features used are:\n",
        "- SOS, EOS, lowercase, uppercase, title, digit, postag, previous_tag, next_tag\n",
        "#### Feel free to include more features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "VctPcQnqbDE9"
      },
      "outputs": [],
      "source": [
        "def token_to_features(doc, i):\n",
        "    word = doc[i][0]\n",
        "    postag = doc[i][1]\n",
        "\n",
        "    # Common features for all words\n",
        "    features = [\n",
        "        'bias',\n",
        "        'word.lower=' + word.lower(),\n",
        "        'word[-3:]=' + word[-3:],\n",
        "        'word[-2:]=' + word[-2:],\n",
        "        'word[:2]=' + word[:2],\n",
        "        'word[:3]=' + word[:3],\n",
        "        'word.isupper=%s' % word.isupper(),\n",
        "        'word.istitle=%s' % word.istitle(),\n",
        "        'word.isdigit=%s' % word.isdigit(),\n",
        "        'word.length=%s' % len(word),\n",
        "        'word.hasdigit=%s' % any(ch.isdigit() for ch in word),\n",
        "        'word.hasdash=%s' % ('-' in word),\n",
        "        'postag=' + postag,\n",
        "        'postag[:2]=' + postag[:2]\n",
        "\n",
        "    ]\n",
        "\n",
        "    # Features for words that are not\n",
        "    # at the beginning of a document\n",
        "    if i > 0:\n",
        "        word1 = doc[i-1][0]\n",
        "        postag1 = doc[i-1][1]\n",
        "        features.extend([\n",
        "            '-1:word.lower=' + word1.lower(),\n",
        "            '-1:word.istitle=%s' % word1.istitle(),\n",
        "            '-1:word.isupper=%s' % word1.isupper(),\n",
        "            '-1:word.isdigit=%s' % word1.isdigit(),\n",
        "            '-1:postag=' + postag1,\n",
        "        ])\n",
        "    else:\n",
        "        # Indicate that it is the 'beginning of a document'\n",
        "        features.append('BOS')\n",
        "\n",
        "    # Features for words that are not\n",
        "    # at the end of a document\n",
        "    if i < len(doc)-1:\n",
        "        word1 = doc[i+1][0]\n",
        "        postag1 = doc[i+1][1]\n",
        "        features.extend([\n",
        "            '+1:word.lower=' + word1.lower(),\n",
        "            '+1:word.istitle=%s' % word1.istitle(),\n",
        "            '+1:word.isupper=%s' % word1.isupper(),\n",
        "            '+1:word.isdigit=%s' % word1.isdigit(),\n",
        "            '+1:postag=' + postag1,\n",
        "            '+1:postag[:2]=' + postag1[:2],\n",
        "        ])\n",
        "    else:\n",
        "        # Indicate that it is the 'end of a document'\n",
        "        features.append('EOS')\n",
        "\n",
        "    return features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VlMdadmGbDE9"
      },
      "source": [
        "### Running the feature extractor on the training data\n",
        "- Feature extraction\n",
        "- Train-test-split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S1FR-3wCbDE9",
        "outputId": "8cf949a8-8ea4-4d99-f74f-75193380ca06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training samples: 44\n",
            "Testing samples: 12\n",
            "Sample features for first sentence:\n",
            "['bias', 'word.lower=qid', 'word[-3:]=qid', 'word[-2:]=id', 'word[:2]=qi', 'word[:3]=qid', 'word.isupper=False', 'word.istitle=False', 'word.isdigit=False', 'word.length=3', 'word.hasdigit=False', 'word.hasdash=False', 'postag=NN', 'postag[:2]=NN', 'BOS', 'EOS']\n",
            "Labels: ['QID']\n"
          ]
        }
      ],
      "source": [
        "tuples_data = tuples_maker(input_sigs, output_labels)\n",
        "train_data_triples = triples_maker(tuples_data)\n",
        "\n",
        "\n",
        "# Helper functions\n",
        "def sent2features(sent):\n",
        "    return [token_to_features(sent, i) for i in range(len(sent))]\n",
        "\n",
        "def sent2labels(sent):\n",
        "    return [label for token, postag, label in sent]\n",
        "\n",
        "X = [sent2features(s) for s in train_data_triples]\n",
        "y = [sent2labels(s) for s in train_data_triples]\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 80% train, 20% test\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "print(f\"Training samples: {len(X_train)}\")\n",
        "print(f\"Testing samples: {len(X_test)}\")\n",
        "\n",
        "print(\"Sample features for first sentence:\")\n",
        "print(X_train[0][0])  # Features for first word\n",
        "print(\"Labels:\", y_train[0])  # Labels for full sentence\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQr2PFKWbDE9"
      },
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "djy7mVKrbDE9",
        "outputId": "02504018-0046-4f92-97f7-52c5b7b715d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training CRF model... \n",
            "Training completed!\n",
            "Model saved successfully\n"
          ]
        }
      ],
      "source": [
        "crf = sklearn_crfsuite.CRF(\n",
        "    algorithm='lbfgs',\n",
        "    c1=0.1,\n",
        "    c2=0.1,\n",
        "    max_iterations=100,\n",
        "    all_possible_transitions=True\n",
        ")\n",
        "\n",
        "\n",
        "print(\"Training CRF model... \")\n",
        "crf.fit(X_train, y_train)\n",
        "print(\"Training completed!\")\n",
        "\n",
        "\n",
        "model_filename = \"prescription_parser_crf_model.pkl\"\n",
        "joblib.dump(crf, model_filename)\n",
        "print(f\"Model saved successfully\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZfeScENbDE-"
      },
      "source": [
        "### Predicting the test data with the built model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "crf_loaded = joblib.load(\"prescription_parser_crf_model.pkl\")\n",
        "\n",
        "\n",
        "y_pred = crf_loaded.predict(X_test)\n",
        "\n",
        "\n",
        "from sklearn_crfsuite import metrics\n",
        "\n",
        "print(\"Model accuracy (F1-score):\",\n",
        "      metrics.flat_f1_score(y_test, y_pred, average='weighted'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzKks3kXhZ-W",
        "outputId": "6822ac2b-cd30-4026-fa4d-8976487efb6a"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model accuracy (F1-score): 0.8731628453850676\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEELIcl1bDE-"
      },
      "source": [
        "### Putting all the prediction logic inside a predict method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "fXR-cfw5bDE-"
      },
      "outputs": [],
      "source": [
        "def predict(sig):\n",
        "    \"\"\"\n",
        "    predict(sig)\n",
        "    Purpose: Labels the given sig into corresponding labels\n",
        "    @param sig: A Sentence (medical prescription sig)\n",
        "    @return: A list of predicted labels\n",
        "    >>> predict('2 tabs every 4 hours')\n",
        "    [['Qty', 'Form', 'EVERY', 'Period', 'PeriodUnit']]\n",
        "    >>> predict('2 tabs with food')\n",
        "    [['Qty', 'Form', 'WITH', 'FOOD']]\n",
        "    >>> predict('2 tabs qid x 30 days')\n",
        "    [['Qty', 'Form', 'QID', 'FOR', 'Duration', 'DurationUnit']]\n",
        "    \"\"\"\n",
        "\n",
        "    tokens = nltk.word_tokenize(sig)\n",
        "\n",
        "\n",
        "    pos_tagged_tokens = nltk.pos_tag(tokens)\n",
        "\n",
        "\n",
        "    formatted_input = [(token, pos) for token, pos in pos_tagged_tokens]\n",
        "\n",
        "\n",
        "    features_for_sig = [token_to_features(formatted_input, i) for i in range(len(formatted_input))]\n",
        "\n",
        "\n",
        "    predictions = crf.predict([features_for_sig])\n",
        "\n",
        "    return predictions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XfqogA5AbDE-"
      },
      "source": [
        "### Sample predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tw0fx_DvbDE_",
        "outputId": "6ee27d4a-048b-4339-e8d9-f2d89669fd86"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['Method' 'Qty' 'Form' 'EVERY' 'Period' 'PeriodUnit' 'FOR' 'Duration'\n",
            "  'DurationUnit']]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')  # For word tokenization\n",
        "nltk.download('averaged_perceptron_tagger')  # For POS tagging\n",
        "predictions = predict(\"take 2 tabs every 6 hours x 10 days\")\n",
        "print(predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MCSg9f7sbDE_",
        "outputId": "5bb49dc2-fd11-4a2d-aa5c-5a043a0ebb2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['Qty' 'Form' 'FOR' 'Duration' 'DurationUnit' 'AT' 'WHEN']]\n"
          ]
        }
      ],
      "source": [
        "predictions = predict(\"2 capsu for 10 day at bed\")\n",
        "print(predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "z8JQqhd0bDFB"
      },
      "outputs": [],
      "source": [
        "predictions = predict(\"inject 3 units orally\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "3-xBAKLnbDFC"
      },
      "outputs": [],
      "source": [
        "predictions = predict(\"by mouth take three caps\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "0XLrxt0gbDFC"
      },
      "outputs": [],
      "source": [
        "predictions = predict(\"take 3 tabs orally three times a day for 10 days at bedtime\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "1ud8099ZbDFC"
      },
      "outputs": [],
      "source": [
        "predictions = predict(\"take 3 tabs orally bid for 10 days at bedtime\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "8lSeU_ZMbDFD"
      },
      "outputs": [],
      "source": [
        "predictions = predict(\"take 10 capsules by mouth qid\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "DHVTVEOibDFD"
      },
      "outputs": [],
      "source": [
        "prediction = predict(\"Amoxcicillin QID 30 tablets\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "qwX8lH92bDFE"
      },
      "outputs": [],
      "source": [
        "prediction = predict(\"take two to four tabs\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "T8TWRcJEbDFF"
      },
      "outputs": [],
      "source": [
        "prediction = predict(\"take 2 tabs every 4 to 6 days\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "HJpKoI6rbDFF"
      },
      "outputs": [],
      "source": [
        "prediction = predict(\"every 10 mins\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "eBWu0lNUbDFG"
      },
      "outputs": [],
      "source": [
        "prediction = predict(\"once in every 3 days take 3 tabs\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7da138f"
      },
      "source": [
        "# Task\n",
        "Train the `sklearn_crfsuite.CRF` model using `X_train` and `y_train`, then predict labels for `X_test` and store them in `y_pred`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3f449054"
      },
      "source": [
        "## Train CRF Model\n",
        "\n",
        "### Subtask:\n",
        "Initialize and train the `sklearn_crfsuite.CRF` model using the prepared `X_train` (features) and `y_train` (labels) data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dac9662e"
      },
      "source": [
        "**Reasoning**:\n",
        "Initialize the CRF model with the specified parameters and then train it using the `X_train` and `y_train` data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3a6e4e3e"
      },
      "source": [
        "crf = sklearn_crfsuite.CRF(\n",
        "    algorithm='lbfgs',\n",
        "    c1=0.1,\n",
        "    c2=0.1,\n",
        "    max_iterations=100,\n",
        "    all_possible_transitions=True\n",
        ")\n",
        "crf.fit(X_train, y_train)\n",
        "print(\"CRF model trained successfully.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1a1da9bc"
      },
      "source": [
        "**Reasoning**:\n",
        "The `NameError` indicates that `X_train` and `y_train` were not defined before attempting to train the CRF model. This means the preceding steps for data preparation, feature extraction, and train-test split were not executed or their results were lost. I need to re-execute these steps to properly define `X_train` and `y_train` before training the model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fd0c0d31",
        "outputId": "94ddbc4a-e9a5-4700-8413-604138348aff"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "processed_data = tuples_maker(input_sigs, output_labels)\n",
        "\n",
        "\n",
        "train_data_triples = triples_maker(processed_data)\n",
        "\n",
        "\n",
        "def sent2features(sent):\n",
        "    return [token_to_features(sent, i) for i in range(len(sent))]\n",
        "\n",
        "def sent2labels(sent):\n",
        "    return [label for token, postag, label in sent]\n",
        "\n",
        "\n",
        "X = [sent2features(s) for s in train_data_triples]\n",
        "y = [sent2labels(s) for s in train_data_triples]\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "crf = sklearn_crfsuite.CRF(\n",
        "    algorithm='lbfgs',\n",
        "    c1=0.1,\n",
        "    c2=0.1,\n",
        "    max_iterations=100,\n",
        "    all_possible_transitions=True\n",
        ")\n",
        "crf.fit(X_train, y_train)\n",
        "print(\"CRF model trained successfully.\")"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CRF model trained successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4397dbb"
      },
      "source": [
        "## Predict on Test Data\n",
        "\n",
        "### Subtask:\n",
        "Use the trained CRF model to make predictions on the `X_test` dataset and store the results in `y_pred`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6d251528"
      },
      "source": [
        "**Reasoning**:\n",
        "Use the trained CRF model to predict labels for the `X_test` dataset and store them in `y_pred`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7dad25d",
        "outputId": "db241771-83a0-4c8c-bdf2-b654940c610f"
      },
      "source": [
        "y_pred = crf.predict(X_test)\n",
        "print(\"Predictions generated for X_test.\")"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions generated for X_test.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6421dcdb"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Complete the task of predicting on the test data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8774b1ac"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   An `sklearn_crfsuite.CRF` model was successfully initialized with `algorithm='lbfgs'`, `c1=0.1`, `c2=0.1`, `max_iterations=100`, and `all_possible_transitions=True`.\n",
        "*   The CRF model was trained using the prepared `X_train` (features) and `y_train` (labels) datasets.\n",
        "*   Predictions were successfully generated for `X_test` using the trained CRF model.\n",
        "*   The predicted labels for the test data were stored in the `y_pred` variable.\n",
        "*   Initially, data preparation and feature extraction steps needed to be re-executed to define `X_train` and `y_train`, resolving a `NameError`.\n",
        "\n",
        "### Insights or Next Steps\n",
        "*   The next logical step is to evaluate the performance of the trained CRF model by comparing the `y_pred` results against the true labels `y_test` using appropriate metrics (e.g., accuracy, precision, recall, F1-score).\n",
        "*   Investigate the features used for training and consider engineering additional features that might improve the model's predictive capabilities, especially if performance metrics are not optimal.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "testing",
      "language": "python",
      "name": "testing"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}